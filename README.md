# Evaluating Image Representations for Video Prediction

A comprehensive implementation of video prediction models using Hybrid Transformer-based and CNN architectures for both **holistic** and **object-centric** scene representations. This project explores different approaches to learning and predicting future video frames on the MOVi-C dataset.

<p align="center">
  <img src="src/experiments/02_Holistic_AE_XL/tboard_logs/recons.gif" alt="Reconstruction GIF" width="600" height="400" />
</p>

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Project Structure](#-project-structure)
- [Architecture](#ï¸-architecture)
- [Dataset](#-dataset)
- [Usage](#-usage)
- [Experiments](#-experiments)
- [Model Checkpoints](#-model-checkpoints)
- [Configuration](#ï¸-configuration)
- [Results](#-results)
- [Citation](#-citation)

## ğŸ¯ Overview

This project implements a two-stage video prediction pipeline:

1. **Stage 1 - Autoencoder Training**: Learn compressed representations of video frames
2. **Stage 2 - Predictor Training**: Predict future frame representations in latent space

The framework supports two distinct scene representation approaches:
- **Holistic Representation**: Treats the entire scene as a unified entity
- **Object-Centric (OC) Representation**: Decomposes scenes into individual objects using masks/bounding boxes

## âœ¨ Features

- ğŸ”„ **Two-Stage Training Pipeline**: Separate autoencoder and predictor training phases
- ğŸ­ **Dual Scene Representations**: Support for both holistic and object-centric approaches
- ğŸ§  **Transformer-Based Architecture**: Modern attention-based encoders and decoders
- ğŸ¯ **Flexible Configuration**: Easy-to-modify configuration system
- ğŸ“Š **Comprehensive Logging**: TensorBoard integration with visualization support
- âš¡ **Mixed Precision Training**: Efficient GPU utilization with AMP support
- ğŸ” **Early Stopping & Scheduling**: Automatic training optimization
- ğŸ’¾ **Checkpoint Management**: Automatic model saving and loading


## ğŸ“ Project Structure

```
CourseProject_2/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ base/                    # Base classes
â”‚   â”‚   â”œâ”€â”€ baseTrainer.py       # Base trainer implementation
â”‚   â”‚   â””â”€â”€ baseTransformer.py   # Base transformer blocks
â”‚   â”œâ”€â”€ datalib/                 # Data loading and processing
â”‚   â”‚   â”œâ”€â”€ MoviC.py            # MOVi-C dataset class
â”‚   â”‚   â”œâ”€â”€ load_data.py        # Data loading utilities
â”‚   â”‚   â””â”€â”€ transforms.py        # Data augmentation
â”‚   â”œâ”€â”€ model/                   # Model architectures
â”‚   â”‚   â”œâ”€â”€ ocvp.py             # Main model definitions (TransformerAutoEncoder, TransformerPredictor, OCVP)
â”‚   â”‚   â”œâ”€â”€ holistic_encoder.py # Holistic encoder (patch-based)
â”‚   â”‚   â”œâ”€â”€ holistic_decoder.py # Holistic decoder
â”‚   â”‚   â”œâ”€â”€ holistic_predictor.py # Holistic predictor
â”‚   â”‚   â”œâ”€â”€ oc_encoder.py       # Object-centric encoder (CNN + Transformer)
â”‚   â”‚   â”œâ”€â”€ oc_decoder.py       # Object-centric decoder (Transformer + CNN)
â”‚   â”‚   â”œâ”€â”€ oc_predictor.py     # Object-centric predictor
â”‚   â”‚   â”œâ”€â”€ predictor_wrapper.py # Autoregressive wrapper with sliding window
â”‚   â”‚   â””â”€â”€ model_utils.py      # Model utilities (TransformerBlock, Patchifier, etc.)
â”‚   â”œâ”€â”€ utils/                   # Utility functions
â”‚   â”‚   â”œâ”€â”€ logger.py           # Logging utilities
â”‚   â”‚   â”œâ”€â”€ metrics.py          # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ utils.py            # General utilities
â”‚   â”‚   â””â”€â”€ visualization.py    # Visualization tools
â”‚   â”œâ”€â”€ experiments/             # Experiment outputs
â”‚   â”‚   â””â”€â”€ [experiment_name]/
â”‚   â”‚       â”œâ”€â”€ checkpoints/    # Model checkpoints
â”‚   â”‚       â”œâ”€â”€ config/         # Experiment config
â”‚   â”‚       â””â”€â”€ tboard_logs/    # TensorBoard logs
â”‚   â”œâ”€â”€ CONFIG.py               # Global configuration
â”‚   â”œâ”€â”€ trainer.py              # Training entry point
â”‚   â””â”€â”€ ocvp.ipynb             # Analysis notebook
â”œâ”€â”€ docs/                       # Documentation and reports
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

### Why Transformer + CNN Hybrid?

The object-centric model uses a **hybrid Transformer + CNN architecture** for optimal performance:

**CNN Advantages:**
- âœ… **Inductive Bias**: Built-in understanding of spatial locality and translation invariance
- âœ… **Efficient Downsampling**: Reduces 64Ã—64 images to compact 256D vectors
- âœ… **Parameter Efficiency**: Fewer parameters than fully linear projections
- âœ… **Better Image Reconstruction**: ConvTranspose layers naturally upsample spatial features

**Transformer Advantages:**
- âœ… **Temporal Modeling**: Captures long-range dependencies across time
- âœ… **Object Relationships**: Models interactions between multiple objects
- âœ… **Attention Mechanism**: Learns which objects/features are important
- âœ… **Flexible Context**: Handles variable number of objects and temporal sequences

**Combined Benefits:**
- ğŸ¯ CNNs handle spatial features (what objects look like)
- ğŸ¯ Transformers handle temporal dynamics (how objects move and interact)
- ğŸ¯ Best of both worlds: local spatial structure + global temporal reasoning

### Key Components

1. **Encoder** (`HolisticEncoder` / `ObjectCentricEncoder`)
   - **Holistic**: Patchifies input images (16Ã—16 patches) â†’ Linear projection â†’ Transformer
   - **Object-Centric**: CNN encoder + Transformer hybrid architecture
     - **CNN Feature Extraction**: 3-layer ConvNet downsampler
       - Conv2d(3â†’64): 64Ã—64 â†’ 32Ã—32
       - Conv2d(64â†’128): 32Ã—32 â†’ 16Ã—16
       - Conv2d(128â†’256): 16Ã—16 â†’ 8Ã—8
       - Linear: Flatten â†’ 256D embedding
     - Extracts per-object features from masks/bboxes (up to 11 objects)
     - Transformer processes object tokens across time
   - Configurable depth (12 layers default)
   - Embedding dimension: 256
   - Multi-head attention (8 heads)
   - MLP size: 1024

2. **Decoder** (`HolisticDecoder` / `ObjectCentricDecoder`)
   - **Holistic**: Transformer â†’ Linear projection â†’ Unpatchify to image
   - **Object-Centric**: Transformer + CNN hybrid architecture
     - Transformer processes latent object representations
     - **CNN Upsampling Decoder**: 3-layer ConvTranspose
       - Linear: 192D â†’ 128Ã—8Ã—8 feature map
       - ConvTranspose2d(128â†’64): 8Ã—8 â†’ 16Ã—16
       - ConvTranspose2d(64â†’32): 16Ã—16 â†’ 32Ã—32
       - ConvTranspose2d(32â†’3): 32Ã—32 â†’ 64Ã—64 RGB
       - Tanh activation for [-1, 1] output range
     - Combines per-object frames back to full scene
   - Configurable depth (8 layers default)
   - Embedding dimension: 192
   - Mixed loss: MSE (0.8) + L1 (0.2)

3. **Predictor** (`HolisticTransformerPredictor` / `ObjectCentricTransformerPredictor`)
   - Predicts future latent representations autoregressively
   - Transformer-based temporal modeling
   - Configurable depth (8 layers default)
   - Embedding dimension: 192
   - Optional residual connections

4. **Predictor Wrapper** (`PredictorWrapper`)
   - **Autoregressive Prediction**: Iteratively predicts future frames
   - **Sliding Window Mechanism**: Maintains a buffer of size 5
     - Concatenates new predictions to input buffer
     - Drops oldest frames when buffer exceeds window size
   - **Training Strategy**:
     - Random temporal slicing for data augmentation
     - Per-step loss computation with temporal consistency
   - **Advanced Loss Function**:
     - MSE loss (0.6): Overall structure
     - L1 loss (0.2): Sharpness and sparsity
     - Cosine similarity loss (0.2): Feature alignment
   - Generates 5 future frame predictions per forward pass




## ğŸ—ï¸ Architecture

### Overall Pipeline

```
Input Video Frames â†’ Encoder â†’ Latent Representation â†’ Predictor â†’ Future Latent â†’ Decoder â†’ Predicted Frames
```

### Detailed Architecture: Object-Centric Model (Transformer + CNN)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          OBJECT-CENTRIC ENCODER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input: Video [B, T, 3, 64, 64] + Masks [B, T, 64, 64]                       â”‚
â”‚   â†“                                                                         â”‚
â”‚ Object Extraction (11 objects max)                                          â”‚
â”‚   â†’ Object Frames: [B, T, 11, 3, 64, 64]                                    â”‚
â”‚   â†“                                                                         â”‚
â”‚ CNN Feature Extractor (Per Object):                                         â”‚
â”‚   â€¢ Conv2d(3â†’64, k=4, s=2) + BatchNorm + ReLU    [64x64 â†’ 32x32]            â”‚
â”‚   â€¢ Conv2d(64â†’128, k=4, s=2) + BatchNorm + ReLU  [32x32 â†’ 16x16]            â”‚
â”‚   â€¢ Conv2d(128â†’256, k=4, s=2) + BatchNorm + ReLU [16x16 â†’ 8x8]              â”‚
â”‚   â€¢ Flatten + Linear(256Â·8Â·8 â†’ 256)                                         â”‚
â”‚   â†’ Object Tokens: [B, T, 11, 256]                                          â”‚
â”‚   â†“                                                                         â”‚
â”‚ Transformer Encoder (12 layers):                                            â”‚
â”‚   â€¢ Positional Encoding                                                     â”‚
â”‚   â€¢ Multi-Head Attention (8 heads, dim=128)                                 â”‚
â”‚   â€¢ MLP (dim=1024)                                                          â”‚
â”‚   â€¢ Layer Normalization                                                     â”‚
â”‚   â†’ Latent: [B, T, 11, 256]                                                 â”‚ 
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            PREDICTOR + WRAPPER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input Latent: [B, T=24, 11, 256]                                            â”‚
â”‚   â†“                                                                         â”‚
â”‚ PredictorWrapper (Autoregressive):                                          â”‚
â”‚   â€¢ Random temporal slice (5 frames)                                        â”‚
â”‚   â€¢ Sliding window buffer (size=5)                                          â”‚
â”‚   â†“                                                                         â”‚
â”‚ Transformer Predictor (8 layers):                                           â”‚
â”‚   â€¢ Linear(256 â†’ 192)                                                       â”‚
â”‚   â€¢ Transformer blocks (depth=8)                                            â”‚
â”‚   â€¢ Linear(192 â†’ 256)                                                       â”‚
â”‚   â€¢ Optional residual connections                                           â”‚
â”‚   â†“                                                                         â”‚
â”‚ Autoregressive Loop (5 predictions):                                        â”‚
â”‚   For t in 1..5:                                                            â”‚
â”‚     â€¢ Predict next frame                                                    â”‚
â”‚     â€¢ Append to buffer, shift window                                        â”‚
â”‚     â€¢ Compute loss (MSE + L1 + Cosine)                                      â”‚
â”‚   â†’ Future Latent: [B, 5, 11, 256]                                          â”‚ 
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          OBJECT-CENTRIC DECODER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input Latent: [B, T, 11, 256]                                               â”‚
â”‚   â†“                                                                         â”‚
â”‚ Transformer Decoder (8 layers):                                             â”‚
â”‚   â€¢ Linear(256 â†’ 192)                                                       â”‚
â”‚   â€¢ Positional Encoding                                                     â”‚
â”‚   â€¢ Transformer blocks (depth=8)                                            â”‚
â”‚   â€¢ Layer Normalization                                                     â”‚
â”‚   â†’ [B, T, 11, 192]                                                         â”‚
â”‚   â†“                                                                         â”‚
â”‚ CNN Upsampling Decoder (Per Object):                                        â”‚
â”‚   â€¢ Linear(192 â†’ 128Â·8Â·8) + Reshape to [128, 8, 8]                          â”‚
â”‚   â€¢ ConvTranspose2d(128â†’64, k=4, s=2) + BatchNorm + ReLU [8x8 â†’ 16x16]      â”‚
â”‚   â€¢ ConvTranspose2d(64â†’32, k=4, s=2) + BatchNorm + ReLU [16x16 â†’ 32x32]     â”‚
â”‚   â€¢ ConvTranspose2d(32â†’3, k=4, s=2) + Tanh        [32x32 â†’ 64x64]           â”‚
â”‚   â†’ Per-Object Frames: [B, T, 11, 3, 64, 64]                                â”‚
â”‚   â†“                                                                         â”‚
â”‚ Object Composition:                                                         â”‚
â”‚   â€¢ Sum all object frames: Î£(objects)                                       â”‚
â”‚   â€¢ Normalize: (x + 1) / 2  (from [-1,1] to [0,1])                          â”‚
â”‚   â€¢ Clamp to [0, 1]                                                         â”‚
â”‚   â†’ Reconstructed Video: [B, T, 3, 64, 64]                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Setup

1. **Clone the repository**:
```bash
git clone <repository-url>
cd CourseProject_2
```

2. **Create and activate virtual environment**:
```bash
python -m venv venv
source venv/bin/activate  # On Linux/Mac
# or
venv\Scripts\activate  # On Windows
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

## ğŸ“¦ Dataset

This project uses the **MOVi-C dataset** (Multi-Object Video Dataset).

### Dataset Setup

1. Download MOVi-C dataset from the official source
2. Extract to your preferred location
3. Update the dataset path in `src/CONFIG.py`:

```python
config = {
    'data': {
        'dataset_path': '/path/to/movi_c/',
        ...
    }
}
```

### Dataset Structure

The MOVi-C dataset should have the following structure:
```
movi_c/
â”œâ”€â”€ train/
â”œâ”€â”€ validation/
â””â”€â”€ test/
```

## ğŸ’» Usage

### Training Autoencoder

Train the autoencoder with holistic representation:

```bash
cd src
python trainer.py --ae --scene_rep holistic
```

Train with object-centric representation:

```bash
python trainer.py --ae --scene_rep oc
```

### Training Predictor

After training the autoencoder, train the predictor:

```bash
python trainer.py --predictor --scene_rep holistic \
    --ackpt experiments/01_Holistic_AE_XL/checkpoints/best_01_Holistic_AE_XL.pth
```

For object-centric:

```bash
python trainer.py --predictor --scene_rep oc \
    --ackpt experiments/01_OC_AE_XL_64_Full_CNN/checkpoints/best_01_OC_AE_XL_64_Full_CNN.pth
```

### Inference

Run end-to-end video prediction:

```bash
python trainer.py --inference --scene_rep holistic \
    --ackpt path/to/autoencoder.pth \
    --pckpt path/to/predictor.pth
```

### Command-Line Arguments

| Argument | Short | Description |
|----------|-------|-------------|
| `--ae` | `-a` | Enable autoencoder training mode |
| `--predictor` | `-p` | Enable predictor training mode |
| `--inference` | `-i` | Enable end-to-end inference mode |
| `--ackpt` | `-ac` | Path to pretrained autoencoder checkpoint |
| `--pckpt` | `-pc` | Path to pretrained predictor checkpoint |
| `--scene_rep` | `-s` | Scene representation type: `holistic` or `oc` |


## ğŸ”¬ Experiments

The project includes several experimental configurations:

### Autoencoder Experiments

1. **Holistic Autoencoders**:
   - `01_Holistic_AE_Base`: Baseline holistic autoencoder
   - `02_Holistic_AE_XL`: Extra-large holistic autoencoder

2. **Object-Centric Autoencoders**:
   - `01_OC_AE_XL_64_Full_CNN`: Full CNN-based OC autoencoder
   - `01_OC_AE_XL_64_Mixed_CNN_Decoder_Linear_ENCODER`: Mixed architecture
   - Various linear and advanced configurations

### Predictor Experiments

1. **Holistic Predictors**:
   - `02_Holistic_Predictor_XL`: Standard predictor
   - `03_Holistic_Predictor_XL`: Improved version
   - `05_Holistic_Predictor_XL_NoResidual`: Without residual connections

2. **Object-Centric Predictors**:
   - `01_OC_Predictor_XL`: Standard OC predictor

### Experiment Outputs

Each experiment generates:
- **Checkpoints**: Best and periodic model saves
- **TensorBoard Logs**: Training curves, visualizations
- **Configuration Snapshots**: Reproducible experiment configs

## ğŸ’¾ Model Checkpoints

Pre-trained model checkpoints are available for download:

ğŸ”— [Download Model Checkpoints](https://drive.google.com/drive/folders/1cS1CwnZP8BhBA2fGcYH9fJmT7s9OIALP?usp=drive_link)

### Available Checkpoints

- Holistic Autoencoder (Base & XL)
- Object-Centric Autoencoder (Various configurations)
- Holistic Predictor (Multiple versions)
- Object-Centric Predictor

## âš™ï¸ Configuration

The main configuration file is `src/CONFIG.py`. Key parameters:

### Data Configuration

```python
'data': {
    'dataset_path': '/path/to/movi_c/',
    'batch_size': 32,
    'patch_size': 16,
    'max_objects': 11,
    'num_workers': 8,
    'image_height': 64,
    'image_width': 64,
}
```

### Training Configuration

```python
'training': {
    'num_epochs': 300,
    'warmup_epochs': 15,
    'early_stopping_patience': 15,
    'model_name': '01_OC_AE_XL_64_Full_CNN',
    'lr': 4e-4,
    'save_frequency': 25,
    'use_scheduler': True,
    'use_early_stopping': True,
    'use_transforms': False,
    'use_amp': True,  # Mixed precision training
}
```

### Model Configuration

```python
'vit_cfg': {
    'encoder_embed_dim': 256,
    'decoder_embed_dim': 192,
    'num_heads': 8,
    'mlp_size': 1024,
    'encoder_depth': 12,
    'decoder_depth': 8,
    'predictor_depth': 8,
    'num_preds': 5,
    'predictor_window_size': 5,
    'use_masks': True,
    'use_bboxes': False,
    'residual': True,
}
```

## ğŸ“Š Results

### Reconstruction Quality

The models achieve high-quality video frame reconstruction:

- **Holistic Models**: Capture global scene structure effectively
- **Object-Centric Models**: Better at preserving individual object details

### Visualization

View results in the Jupyter notebook:

```bash
cd src
jupyter lab ocvp.ipynb
```

The notebook includes:
- Training/validation loss curves
- Reconstruction visualizations
- Prediction quality analysis
- Comparison between holistic and object-centric approaches

### TensorBoard

Monitor training progress:

```bash
tensorboard --logdir src/experiments/[experiment_name]/tboard_logs
```

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@misc{video_prediction_ocvp,
  title={Evaluating Image Representations for Video Prediction},
  author={Your Name},
  year={2025},
  howpublished={\url{https://github.com/your-repo}}
}
```

---

**Note**: This is a course project Video Prediction with Object Representations. See the `docs/` folder for project reports and lab notebook examples.
